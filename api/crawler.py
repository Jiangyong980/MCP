import requests
from bs4 import BeautifulSoup
import json
import time
import re

# === é…ç½®åŒºåŸŸ ===
BASE_URL = "http://172.16.0.193:8088"
USERNAME = "your_username"
PASSWORD = "your_password"

class ZentaoDevDocCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.base_url = BASE_URL.rstrip('/')
        self.docs = []

    def login(self):
        """ç™»å½•ç¦…é“ä»¥è·å–æƒé™"""
        # æ³¨æ„ï¼šè¿™é‡Œæ¨¡æ‹Ÿæ™®é€šç™»å½•ï¼Œå› ä¸º dev æ¨¡å—é€šå¸¸ä¾èµ– Session 
        login_page = f"{self.base_url}/index.php?m=user&f=login"
        res = self.session.get(login_page)
        
        # å°è¯•é€šè¿‡ API token æ–¹å¼æˆ–æ¨¡æ‹Ÿè¡¨å•ç™»å½•
        # é‰´äºä½ ä¹‹å‰ token æˆåŠŸäº†ï¼Œæˆ‘ä»¬å…ˆå°è¯•æŠŠ token æ”¾å…¥ Cookie
        # å¦‚æœè¿˜æ˜¯æç¤ºéœ€è¦ç™»å½•ï¼Œå»ºè®®åœ¨æµè§ˆå™¨ F12 æŸ¥çœ‹ zentaosid å¹¶æ‰‹åŠ¨å¡«å…¥
        login_api = f"{self.base_url}/api.php/v1/tokens"
        payload = {"account": USERNAME, "password": PASSWORD}
        res = self.session.post(login_api, json=payload)
        if res.status_code in [200, 201]:
            token = res.json().get('token')
            self.session.headers.update({"Token": token})
            print("âœ… ç™»å½•éªŒè¯æˆåŠŸ")
            return True
        return False

    def get_api_list(self):
        """ä»ä¸»é¡µè·å–æ‰€æœ‰çš„ apiID"""
        list_url = f"{self.base_url}/index.php?m=dev&f=api"
        print(f"æ­£åœ¨è·å–æ¥å£ç›®å½•: {list_url}")
        res = self.session.get(list_url)
        
        # ä½¿ç”¨æ­£åˆ™æˆ– BeautifulSoup åŒ¹é…æ‰€æœ‰çš„ apiID=xxx
        # ç¦…é“çš„é“¾æ¥æ ¼å¼é€šå¸¸æ˜¯ apiID=123
        ids = re.findall(r'apiID=(\d+)', res.text)
        unique_ids = sorted(list(set(ids)), key=int)
        print(f"ğŸ“‚ å‘ç° {len(unique_ids)} ä¸ªæ¥å£å®šä¹‰")
        return unique_ids

    def parse_api_detail(self, api_id):
        """çˆ¬å–å•ä¸ª API çš„è¯¦ç»†ä¿¡æ¯"""
        url = f"{self.base_url}/index.php?m=dev&f=api&module=restapi&apiID={api_id}&zin=1"
        try:
            res = self.session.get(url)
            if res.status_code != 200:
                return None
            
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # æå–ä¿¡æ¯ (æ ¹æ®ç¦…é“ç•Œé¢çš„ HTML ç»“æ„æå–)
            # è¿™é‡Œçš„æå–é€»è¾‘éœ€è¦æ ¹æ®ä½ çœ‹åˆ°çš„é¡µé¢æºç å¾®è°ƒ
            title = soup.find('h2')
            title_text = title.get_text(strip=True) if title else f"API_{api_id}"
            
            # æå–è¡¨æ ¼ä¸­çš„å‚æ•°ã€URLã€è¯·æ±‚æ–¹æ³•ç­‰
            content = soup.get_text(separator='\n', strip=True)
            
            print(f"  - å·²æŠ“å–: [{api_id}] {title_text}")
            
            return {
                "id": api_id,
                "title": title_text,
                "url": url,
                "full_content": content # ä¿å­˜å…¨æ–‡ï¼Œæ–¹ä¾¿åç»­æœç´¢
            }
        except Exception as e:
            print(f"  - âŒ æŠ“å– ID {api_id} å‡ºé”™: {e}")
            return None

    def run(self):
        if not self.login():
            print("âŒ ç™»å½•å¤±è´¥ï¼Œè¯·æ£€æŸ¥è´¦å·å¯†ç ")
            return

        api_ids = self.get_api_list()
        if not api_ids:
            # å¦‚æœæ²¡æŠ“åˆ° ID åˆ—è¡¨ï¼Œå¯ä»¥å°è¯•æ‰‹åŠ¨ç»™ä¸ªèŒƒå›´è¿›è¡Œæš´åŠ›æ¢æµ‹
            print("âš ï¸ æœªèƒ½ä»ç›®å½•æŠ“å–åˆ° IDï¼Œå°è¯•æš´åŠ›æ¢æµ‹å‰ 50 ä¸ª ID...")
            api_ids = [str(i) for i in range(1, 51)]

        for aid in api_ids:
            data = self.parse_api_detail(aid)
            if data:
                self.docs.append(data)
            time.sleep(0.5)  # ç¨å¾®åœé¡¿ï¼Œé¿å…è¯·æ±‚è¿‡å¿«

        # ä¿å­˜ç»“æœ
        with open("zentao_dev_docs_all.json", "w", encoding="utf-8") as f:
            json.dump(self.docs, f, ensure_ascii=False, indent=4)
        print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼å…±æŠ“å– {len(self.docs)} ä¸ªæ¥å£æ–‡æ¡£ï¼Œä¿å­˜è‡³ zentao_dev_docs_all.json")

if __name__ == "__main__":
    crawler = ZentaoDevDocCrawler()
    crawler.run()